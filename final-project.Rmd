---
title: "Machine Learning Project"
author: "Pedro Márquez"
date: "Monday, July 20, 2015"
output: html_document
---

##Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


##Data 

The training data for this project are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are available here: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.

##What you should submit

The goal of your project is to predict the manner in which they did the exercise. This is the "__classe__" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases. 

1. Your submission should consist of a link to a Github repo with your R markdown and compiled HTML file describing your analysis. Please constrain the text of the writeup to < 2000 words and the number of figures to be less than 5. It will make it easier for the graders if you submit a repo with a gh-pages branch so the HTML page can be viewed online (and you always want to make it easy on graders :-).
2. You should also apply your machine learning algorithm to the 20 test cases available in the test data above. Please submit your predictions in appropriate format to the programming assignment for automated grading. See the programming assignment for additional details. 

##Reproducibility 

Due to security concerns with the exchange of R code, your code will not be run during the evaluation by your classmates. Please be sure that if they download the repo, they will be able to view the compiled HTML version of your analysis. 

###Load all required libraries:
```{r, echo=TRUE}
library(caret)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
```

In order to get the results here reported, set seed as follows:
```{r, echo=TRUE}
set.seed(7531)
```

Load the training and test data sets, taking into consideration that data has different strings we interprete as __NA__:
```{r, echo=TRUE}
trainData <- read.csv("pml-training.csv",
                      na.strings=c("NA","#DIV/0!",""))

testData <- read.csv("pml-testing.csv", 
                     na.strings=c("NA","#DIV/0!",""))

dim(trainData)
dim(testData)
```

###Cleaning the data

Before doing any prediction, we have to clean the data.


__NearZeroVariance__ Variables: Datasets come sometimes with predictors that take an unique value across samples. This kind of predictor is not only non-informative, it can break some models you may want to fit to your data. Even more common is the presence of predictors that are almost constant across samples. One quick and dirty solution is to remove all predictors that satisfy some threshold criterion related to their variance.

```{r, echo=TRUE}
TrainDataNZV <- nearZeroVar(trainData, saveMetrics=TRUE)
head(TrainDataNZV,10)
```

By default, a predictor is classified as near-zero variance if the percentage of unique values in the samples is less than {10\%} and when the frequency ratio mentioned above is greater than 19 (95/5). These default values can be changed by setting the arguments __uniqueCut__ and __freqCut__.

We can explore which ones are the zero variance predictors, and which ones are the near-zero variance predictors:

```{r,echo=TRUE}
TrainDataNZV[TrainDataNZV[,"zeroVar"] + TrainDataNZV[,"nzv"] > 0,]
```

There are `r sum(TrainDataNZV[,"zeroVar"])` __zeroVar__ predictors, and `r sum(TrainDataNZV[,"nzv"])` __nzv__ predictors.

Now, we delete near-zero variance variables, columns not useful, and all columns with too many "NA"s (more than 70%):

```{r,echo=TRUE}
trainData <- trainData[-c(1:7)]
TrainDataNZV <- nearZeroVar(trainData)
newTrainData <- trainData[,-TrainDataNZV]

NAs <- sapply(colnames(newTrainData), 
              function(x) 
                  if(sum(is.na(newTrainData[, x])) > 
                     0.7 * nrow(newTrainData))
                        return(TRUE)
                    else
                        return(FALSE)
              )

cat("We found",sum(NAs),"variables with too many NAs")
newTrainData <- newTrainData[, !NAs]
cat("Final training data dimensions:",dim(newTrainData))

#variable at position 53 is 'classe'
finalTest <- subset(testData, select=colnames(newTrainData[,-53]))
```

We partition the training data to get the test data from it:
```{r, echo=TRUE}
part <- createDataPartition(y=newTrainData$classe, p=0.6, list=FALSE)
newTestData <- newTrainData[-part, ]
newTrainData <- newTrainData[part, ]
dim(newTrainData)
dim(newTrainData)
```

###Identifying Correlated Predictors

While there are some models that thrive on correlated predictors (such as pls), other models may benefit from reducing the level of correlation between the predictors.

Spearman's coefficient, like any correlation calculation, is appropriate for both continuous and discrete variables, including ordinal variables. The Spearman correlation increases in magnitude as __X__ and __Y__ become closer to being perfect monotone functions of each other.

```{r,echo=TRUE}
nums <- sapply(newTrainData, is.numeric)
M <- cor(newTrainData[,nums],
       as.numeric(newTrainData$classe),method="spearman")
print(M)
plot(M, xlab="Index of variable", 
     ylab="Correlation Value",
     main="Correlation Graph")
```


It becomes apparent from the graph and values that there are not predictors that strongly correlate with `classe`, so we will explore other methods.

###Decision Trees
Decision trees helps us explore the stucture of data, while developing easy to visualize decision rules for predicting a categorical classification tree outcome. We look for the tree with the highest cross-validated error less than the minimum cross-validated error plus the standard deviation of the error at that tree. 

```{r,echo=TRUE}
mycontrol = rpart.control(cp = 0, xval = 10)
fittree = rpart(classe ~., method = "class",
     data = newTrainData, control = mycontrol)
minXerror <- min(fittree$cptable[,"xerror"])
row <- fittree$cptable[which(fittree$cptable[,"xerror"]==minXerror),]
newcpt <- row[1] + minXerror
cptrows <- fittree$cptable[which(fittree$cptable[,"CP"] <= newcpt),]
maxmincpt <- max(cptrows[,"CP"])
nrow <- which(fittree$cptable[,"CP"] == maxmincpt)    
cptarg <- sqrt(fittree$cptable[nrow,1]*fittree$cptable[nrow+1,1])
prunedtree = prune(fittree,cp=cptarg)
```

We now prune the decision tree at the best `complexity parameter`:
```{r,echo=TRUE}
DT.fit <- rpart(classe ~ ., data=newTrainData, method="class", cp=cptarg)
fancyRpartPlot(DT.fit)
printcp(DT.fit)
plotcp(DT.fit)
```

Now let's do the predictions:
```{r, echo=TRUE}
pred.DT <- predict(DT.fit, newTestData, type = "class")
confusionMatrix(pred.DT, newTestData$classe)
```

###Boosting
Boosting is a machine learning ensemble meta-algorithm for reducing bias primarily and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. A weak learner is defined to be a classifier which is only slightly correlated with the true classification. In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.

```{r,echo=TRUE}
set.seed(7531)
fitControl <- trainControl(method = "cv",
                           number = 10,
                           repeats = 10)
gbmFit <- train(classe ~ ., data = newTrainData,
                 method = "gbm",
                 trControl = fitControl,
                 verbose = FALSE)
gbmFit
trellis.par.set(caretTheme())
plot(gbmFit)
```

##Random Forest
 Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.
 
```{r, echo=TRUE}
set.seed(7531)
RF.fit <- randomForest(classe ~. , data=newTrainData, method="class")

layout(matrix(c(1,2),nrow=1), width=c(4,1)) 
par(mar=c(5,4,4,0)) #No margin on the right side
plot(RF.fit, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(RF.fit$err.rate),col=1:4,cex=0.8,fill=1:4)
```
Let's do the prediction with random forest:
```{r, echo=TRUE}
pred.RF <- predict(RF.fit, newTestData, type="class")
#confusionMatrix(pred.RF, newTestData$classe)
```

##Files to submit
It seems that Random Forest performed better, as expected, so we use it to submit the files:

```{r,echo=TRUE}
pred.RF <- predict(RF.fit, finalTest, type="class")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,
                quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(pred.RF)
```
